{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# CanvasData - Generate Engagement Data\n",
       
        "***This notebook [currently] assumes you have a CanvasData Spark DB as created by the syncCanvasData pipeline.***\n",
        "\n",
        "*Authors: Departnemt of Education, Tasmania.*\n",
        "\n",
        "\n",
        "Sample notebook that generates a dataset describing course engagement by user and day of year.\n",
        "\n",
        "You can complement this with another dataset (e.g. student and school enrolment data, or network information to identify at-school vs at-home requests).\n",
        "\n",
        "Engagement in this instance is looking at a number of measures including:\n",
        "- Number of requests (web interactions) performed by the user against the course. Note these can happen in the background if someone e.g. leaves Canvas open.\n",
        "- Number of Conversations / Messages against the course.\n",
        "- Number of Quiz Responses\n",
        "- Number of Assignment(s)\n",
        "- Boolean flag \"Participated\" that is True if any of the above conditions are greater than zero.\n",
        "\n",
        "> **Note**: This is not intended as a complete list of engagement measures. You may wish to modify this notebook to suit your own needs (and ideally submit the changes on GitHub :-) )\n",
        "\n",
        "Parameters:\n",
        "- **search_start_date**: Start date for request/engagement data. Data prior to this date will be ignored. Used to manage size of the generated dataset.\n",
        "- **storage_account**: Short name of your storage account where data should be located.\n",
        "- **container_name**: Name of the container to store the data in.\n",
        "- **container_path**: Path to store the new dataset in.\n",
        "- **create_spark_table**: Boolean to indicate whether a Spark table should be created.\n",
        "- **refresh_spark_table**: Whether to refresh spark metadata or not. Useful if the schema has changed (e.g. you've added a column)\n",
        "- **spark_db**: Name of the Spark database.\n",
        "- **spark_table_name**: Name of the Spark table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Notebook Parameters\n",
        "\n",
        "Change these to suite your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "search_start_date = \"2022-01-01\" # Start date for request / other data. Used to reduce size of the dataset.\n",
        "\n",
        "storage_account = '' # Short name of your storage account FQDN (e.g. everything before the dot)\n",
        "container_name = \"stage3np\" # Container to store the data. Typically this won't change\n",
        "container_path = \"VleCourseActivity\" # Path in container to store the data against. \n",
        "\n",
        "create_spark_table = False # Set to TRUE to create a spark table representing the dataset.\n",
        "refresh_spark_table = True # Set to TRUE to refresh table metadata on completion.\n",
        "spark_db = \"VleData\" # Name of your Spark DB where the table should reside. Will be created if it doesn't exist.\n",
        "spark_table_name = \"CanvasData_UserCourseEngagement\" # Name of the table you want to create."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prepare SparkSQL\n",
        "\n",
        "> No changes should be required past this point\n",
        "\n",
        "Here we create several temporary views (session-scoped) to improve readability of the main query further below.\n",
        "\n",
        "CTE's are an alternative to this but are only supported in later versions of SparkSQL (v3 and up)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "\n",
        "# This is just used to surface PySpark parameters in SparkSQL\n",
        "sqlContext.sql(f\"SET search_start_date = '{search_start_date}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Workaround for some schema mapping issues in SparkSQL when reading Parquet.\n",
        "SET spark.sql.legacy.parquet.int96RebaseModeInRead=CORRECTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Course information\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwCourses AS (\n",
        "    SELECT DISTINCT id FROM CanvasData.course_dim cd\n",
        "    WHERE workflow_state = \"available\"\n",
        "    AND   created_at >= ${search_start_date}\n",
        ");\n",
        "\n",
        "-- Aggregation of request data\n",
        "-- Note this may need additional filters to exclude e.g. mobile app based traffic etc. which typically presents as an API call.\n",
        "-- We may also want to break this down by request type (e.g. GET vs PUT/POST [suggesting interaction vs a submission])\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwRequestAgg AS (\n",
        "    SELECT req.user_id, req.course_id, req.timestamp_day, COUNT(course_id) AS request_count\n",
        "    FROM (SELECT * FROM CanvasData.requests WHERE course_id IN (SELECT id FROM vwCourses) AND timestamp_day >= ${search_start_date}) req\n",
        "    WHERE course_id IS NOT NULL\n",
        "    GROUP BY req.user_id, req.course_id, req.timestamp_day\n",
        ");\n",
        "\n",
        "-- Messages associated with a course.\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwConvoMessage AS (\n",
        "    SELECT p.user_id, p.course_id, date_format(msg.created_at, \"yyyy-MM-dd\") AS created_day, COUNT(*) AS message_count\n",
        "    FROM (SELECT * FROM CanvasData.conversation_message_participant_fact WHERE course_id IN (SELECT id FROM vwCourses)) p\n",
        "    INNER JOIN CanvasData.conversation_message_dim msg ON p.conversation_message_id = msg.id\n",
        "    GROUP BY user_id, course_id, date_format(msg.created_at, \"yyyy-MM-dd\")\n",
        ");\n",
        "\n",
        "-- Quiz submissions\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwQuizSubmission AS (\n",
        "    SELECT user_id, course_id, date_format(date, \"yyyy-MM-dd\") AS date, SUM(total_attempts) AS quiz_submission_count\n",
        "    FROM CanvasData.quiz_submission_historical_fact\n",
        "    WHERE course_id IN (SELECT id FROM vwCourses)\n",
        "    GROUP BY user_id, course_id, date_format(date, \"yyyy-MM-dd\")\n",
        ");\n",
        "\n",
        "-- General assignment submissions\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwSub AS (\n",
        "    SELECT sf.user_id, sf.course_id, date_format(sd.submitted_at, \"yyyy-MM-dd\") AS submitted_at_day, COUNT(*) AS submission_count\n",
        "    FROM CanvasData.submission_fact sf\n",
        "    INNER JOIN CanvasData.submission_dim sd ON sf.submission_id = sd.id\n",
        "    WHERE course_id IN (SELECT id FROM vwCourses)\n",
        "    GROUP BY sf.user_id, sf.course_id, date_format(sd.submitted_at, \"yyyy-MM-dd\")\n",
        ");\n",
        "\n",
        "-- List of days in the period\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwDayList AS (\n",
        "    SELECT explode(sequence(search_start_date, current_date(), interval 1 day)) as date\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Dataset Generation\n",
        "\n",
        "Finally - generate the dataset and write into nominated location.\n",
        "\n",
        "Note this is done with PySpark (and not SparkSQL) so we can dynamically set location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE TEMPORARY VIEW vwDataSet AS (\n",
        "    SELECT \n",
        "        pseu.unique_name AS user_name,\n",
        "        u.Name AS user_display_name,\n",
        "        c.Name AS course_name,\n",
        "        req.course_id, \n",
        "        dl.date, \n",
        "        req.request_count AS web_request_count,\n",
        "        COALESCE(msg.message_count, 0) AS message_count,\n",
        "        COALESCE(qz.quiz_submission_count, 0) AS quiz_submission_count,\n",
        "        COALESCE(sub.submission_count, 0) AS submission_count,\n",
        "        CASE WHEN req.request_count > 0 OR COALESCE(msg.message_count, 0) > 0 OR COALESCE(qz.quiz_submission_count, 0) > 0 OR COALESCE(sub.submission_count, 0) > 0\n",
        "            THEN TRUE \n",
        "            ELSE FALSE \n",
        "        END AS participated\n",
        "    FROM CanvasData.user_dim u\n",
        "    INNER JOIN CanvasData.pseudonym_dim pseu ON u.id = pseu.user_id AND pseu.workflow_state = 'active' AND pseu.position = 1\n",
        "    CROSS JOIN vwDayList dl\n",
        "    LEFT OUTER JOIN vwRequestAgg req ON u.id = req.user_id AND req.timestamp_day = dl.date\n",
        "    LEFT OUTER JOIN CanvasData.course_dim c ON req.course_id = c.id\n",
        "    LEFT OUTER JOIN vwConvoMessage msg ON u.id = msg.user_id AND req.course_id = msg.course_id AND req.timestamp_day = msg.created_day\n",
        "    LEFT OUTER JOIN vwQuizSubmission qz ON u.id = qz.user_id AND req.course_id = qz.course_id AND req.timestamp_day = qz.date\n",
        "    LEFT OUTER JOIN vwSub sub ON u.id = sub.user_id AND req.course_id = sub.course_id AND req.timestamp_day = sub.submitted_at_day\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "\n",
        "dest_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{container_path}/\"\n",
        "\n",
        "df = spark.sql(\"SELECT * FROM vwDataSet\")\n",
        "\n",
        "df.write.mode('overwrite').parquet(dest_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "\n",
        "table_name = f\"{spark_db}.{spark_table_name}\"\n",
        "\n",
        "# Create our Spark Table if required.\n",
        "if create_spark_table:\n",
        "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {spark_db}\")\n",
        "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING PARQUET LOCATION '{dest_path}'\")\n",
        "\n",
        "if refresh_spark_table:\n",
        "    spark.sql(f\"REFRESH TABLE {table_name}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
