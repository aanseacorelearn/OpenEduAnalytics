{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%run /OEA_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "139",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-01T18:42:42.0979654Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-01T18:46:14.2233179Z",
              "execution_finish_time": "2022-11-01T18:46:14.2238804Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 139, -1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 80,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the below parameters from pipeline. \r\n",
        "directory = 'Ed-Fi'\r\n",
        "api_version = '5.2'\r\n",
        "metadata_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Ed-Fi/docs/edfi_oea_metadata.csv'\r\n",
        "swagger_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main//modules/module_catalog/Ed-Fi/docs/edfi_swagger.json'\r\n",
        "\r\n",
        "oea = OEA()\r\n",
        "oea_metadatas = oea.get_metadata_from_url(metadata_url)\r\n",
        "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\r\n",
        "\r\n",
        "schema_gen = OpenAPIUtil(swagger_url)\r\n",
        "schemas = schema_gen.create_spark_schemas()\r\n",
        "\r\n",
        "stage2_ingested = oea.to_url(f'stage2/Ingested/{directory}/v{api_version}')\r\n",
        "stage2_refined = oea.to_url(f'stage2/Refined/{directory}/v{api_version}')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p1sm",
              "session_id": "139",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-01T18:47:49.2889887Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-01T18:47:49.5438034Z",
              "execution_finish_time": "2022-11-01T18:47:50.2445333Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p1sm, 139, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-01 18:47:49,529 - OEA - INFO - OEA initialized.\n"
          ]
        }
      ],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_descriptor_schema(descriptor):\r\n",
        "    fields = []\r\n",
        "    fields.append(StructField('_etag',LongType(), True))\r\n",
        "    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\r\n",
        "    fields.append(StructField('codeValue',StringType(), True))\r\n",
        "    fields.append(StructField('description',StringType(), True))\r\n",
        "    fields.append(StructField('id',StringType(), True))\r\n",
        "    fields.append(StructField('namespace',StringType(), True))\r\n",
        "    fields.append(StructField('shortDescription',StringType(), True))\r\n",
        "    return StructType(fields)\r\n",
        "\r\n",
        "def get_descriptor_metadata(descriptor):\r\n",
        "    return [['_etag', 'long', 'no-op'],\r\n",
        "            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\r\n",
        "            ['codeValue','string', 'no-op'],\r\n",
        "            ['description','string', 'no-op'],\r\n",
        "            ['id','string', 'no-op'],\r\n",
        "            ['namespace','string', 'no-op'],\r\n",
        "            ['shortDescription','string', 'no-op']]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p1sm",
              "session_id": "139",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-01T18:47:55.2435002Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-01T18:47:55.3694596Z",
              "execution_finish_time": "2022-11-01T18:47:55.5736434Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p1sm, 139, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_reference_col(df, target_col):\r\n",
        "    col_prefix = target_col.name.replace('Reference', '')\r\n",
        "    \"\"\"for sub_col in [x for x in target_col.dataType.names if x != 'link']:\r\n",
        "        df = df.withColumn(f\"{col_prefix}_{sub_col}\", f.col(f\"{target_col.name}.{sub_col}\"))\"\"\"\r\n",
        "    df = df.withColumn(f\"{col_prefix}_lake_id\", f.concat_ws('_', f.col('SchoolYear')\\\r\n",
        "                              , f.col('DistrictId')\\\r\n",
        "                              , f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3)))\r\n",
        "    df = df.drop(target_col.name)\r\n",
        "    return df\r\n",
        "\r\n",
        "def explode_arrays(df, target_col, schema_name, table_name):\r\n",
        "    cols = ['lake_id', 'DistrictId', 'SchoolYear']\r\n",
        "    child_df = df.select(cols + [target_col.name])\r\n",
        "    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\r\n",
        "    for ref_col in [x for x in child_df.columns if re.search('Reference$', x) is not None]:\r\n",
        "        child_df = flatten_reference_col(child_df, target_col.dataType.elementType[ref_col])\r\n",
        "    \r\n",
        "    for array_sub_col in [x.name for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\r\n",
        "        cols_to_include = [x for x in child_df.columns if '_lake_id' in x] + cols + [array_sub_col]\r\n",
        "        array_child_df = child_df.select(cols_to_include)\r\n",
        "        array_child_df = array_child_df.withColumn('array_exploded', f.explode(array_sub_col)).select(cols_to_include + ['array_exploded.*']).drop(array_sub_col)\r\n",
        "    \r\n",
        "        array_child_df.withColumnRenamed('lake_id', f\"{table_name}_lake_id\").write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
        "                .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}_{array_sub_col}\")\r\n",
        "        child_df = child_df.drop(array_sub_col)\r\n",
        "    child_df.withColumnRenamed('lake_id', f\"{table_name}_lake_id\").write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
        "                .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}\")\r\n",
        "    df = df.drop(target_col.name)\r\n",
        "    return df\r\n",
        "\r\n",
        "        \r\n",
        "def transform(df, target_schema, schema_name, table_name):\r\n",
        "    if re.search('Descriptor$', table_name) is not None:\r\n",
        "        df = df.withColumn('lake_id', f.concat(f.col('SchoolYear'), f.lit('_'), f.col('DistrictId')\\\r\n",
        "                , f.lit('_'), f.col('namespace'), f.lit('#'), f.col('codeValue')))\r\n",
        "    else:\r\n",
        "        df = df.withColumn('lake_id', f.concat_ws('_', f.col('SchoolYear'), f.col('DistrictId'), f.col('id')))\r\n",
        "    \r\n",
        "    for col_name in target_schema.fieldNames():\r\n",
        "        target_col = target_schema[col_name]\r\n",
        "        if col_name in df.columns and target_col.dataType.typeName() in primitive_datatypes:\r\n",
        "            # Primitive data types\r\n",
        "            df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\r\n",
        "            continue\r\n",
        "        elif col_name not in df.columns:\r\n",
        "            # If Column not present in dataframe, Add column with None values.\r\n",
        "            df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
        "            continue\r\n",
        "        \r\n",
        "        df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name)))\\\r\n",
        "                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType))\\\r\n",
        "                    .drop(f\"{col_name}_json\")\r\n",
        "        \r\n",
        "        # Modify the links with surrogate keys\r\n",
        "        if re.search('Reference$', col_name) is not None:\r\n",
        "            df = flatten_reference_col(df, target_col)\r\n",
        "        \r\n",
        "        if target_col.dataType.typeName() == 'array':\r\n",
        "            df = explode_arrays(df, target_col, schema_name, table_name)\r\n",
        "    return df\r\n",
        "#df = transform(df, target_schema, schema_name, table_name)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p1sm",
              "session_id": "139",
              "statement_id": 53,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-01T20:54:21.6003479Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-01T20:54:21.7457384Z",
              "execution_finish_time": "2022-11-01T20:54:21.9160481Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p1sm, 139, 53, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 132,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for schema_name in [x.name for x in mssparkutils.fs.ls(stage2_ingested) if x.isDir]:\r\n",
        "    print(f\"Processing schema: {schema_name}\")\r\n",
        "    \r\n",
        "    for table_name in [y.name for y in mssparkutils.fs.ls(f\"{stage2_ingested}/{schema_name}\") if y.isDir]:\r\n",
        "        print(f\"Processing schema/table: {schema_name}/{table_name}\")\r\n",
        "    \r\n",
        "        # 1. Read Delta table from Ingested Folder.\r\n",
        "        df = spark.read.format('delta').load(f\"{stage2_ingested}/{schema_name}/{table_name}\")\r\n",
        "        if len(df.columns) == 3:\r\n",
        "            print(f\"No Data to process.\")\r\n",
        "            continue\r\n",
        "\r\n",
        "        # 2. Transformation step\r\n",
        "        if(re.search('Descriptors$', table_name) is None):\r\n",
        "            target_schema = schemas[table_name]\r\n",
        "            oea_metadata = oea_metadatas[table_name]\r\n",
        "        else:\r\n",
        "            target_schema = get_descriptor_schema(table_name)\r\n",
        "            oea_metadata = get_descriptor_metadata(table_name)\r\n",
        "        target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\r\n",
        "                                     .add(StructField('SchoolYear', StringType()))\\\r\n",
        "                                     .add(StructField('LastModifiedDate', TimestampType()))\r\n",
        "        oea_metadata += [['DistrictId', 'string', 'partition-by'],\r\n",
        "                        ['SchoolYear', 'string', 'partition-by'],\r\n",
        "                        ['LastModifiedDate', 'timestamp', 'no-op']]\r\n",
        "        try:\r\n",
        "            df = transform(df, target_schema, schema_name, table_name)\r\n",
        "        except:\r\n",
        "            print(f\"Error while Transforming {schema_name}/{table_name}\")\r\n",
        "        \r\n",
        "        # 3. Pseudonymize the data using metadata.\r\n",
        "        try:\r\n",
        "            df_pseudo, df_lookup = oea.pseudonymize(df, oea_metadata)\r\n",
        "        except:\r\n",
        "            print(f\"Error while Pseudonymizing {schema_name}/{table_name}\")\r\n",
        "\r\n",
        "        # 4. Write to Refined folder.\r\n",
        "        if(len(df_pseudo.columns) > 2):\r\n",
        "            df_pseudo.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/General/{schema_name}/{table_name}\")\r\n",
        "        if(len(df_lookup.columns) > 2):\r\n",
        "            df_lookup.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/Sensitive/{schema_name}/{table_name}\")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}