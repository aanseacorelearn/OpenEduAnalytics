{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import datetime\r\n",
        "\r\n",
        "class Intune(BaseOEAModule):\r\n",
        "    def __init__(self, source_folder='intune'):\r\n",
        "        BaseOEAModule.__init__(self, source_folder)\r\n",
        "\r\n",
        "        self.stage1np_devices = self.stage1np + '/devices'\r\n",
        "\r\n",
        "        self.schemas['devices'] = [['DeviceName', 'string', 'no-op'],\r\n",
        "                                    ['ManagedBy', 'string', 'no-op'],\r\n",
        "                                    ['Ownership', 'string', 'no-op'],\r\n",
        "                                    ['Compliance','string','no-op'],\r\n",
        "                                    ['OS', 'string', 'no-op'],\r\n",
        "                                    ['OSVersion', 'string', 'no-op'],\r\n",
        "                                    ['LastCheckIn', 'timestamp', 'no-op'],\r\n",
        "                                    ['PrimaryUserUPN', 'string', 'hash'],\r\n",
        "                                    ['DeviceID', 'string', 'no-op'],\r\n",
        "                                    ['ReportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['devices_enriched'] = [['DeviceName', 'string', 'no-op'],\r\n",
        "                                            ['ManagedBy', 'string', 'no-op'],\r\n",
        "                                            ['Ownership', 'string', 'no-op'],\r\n",
        "                                            ['Compliance','string','no-op'],\r\n",
        "                                            ['OS', 'string', 'no-op'],\r\n",
        "                                            ['OSVersion', 'string', 'no-op'],\r\n",
        "                                            ['LastCheckIn', 'timestamp', 'no-op'],\r\n",
        "                                            ['PrimaryUserUPN', 'string', 'hash'],\r\n",
        "                                            ['DeviceID', 'string', 'no-op'],\r\n",
        "                                            ['LastCheckInDate', 'date', 'no-op'],\r\n",
        "                                            ['AccessOutsideOfSchool', 'boolean', 'no-op'],\r\n",
        "                                            ['ReportYearMonth', 'string', 'partition-by']]\r\n",
        "    \r\n",
        "    def ingest(self):\r\n",
        "        \"\"\" Processes intune data from stage1 into stage2 using structured streaming within the defined function below. \"\"\"\r\n",
        "        logger.info(\"Processing intune data from: \" + self.stage1np)\r\n",
        "\r\n",
        "        items = mssparkutils.fs.ls(self.stage1np)\r\n",
        "        for item in items:\r\n",
        "            # this module only provides defined processing for the intune devices report\r\n",
        "            if item.name == \"devices\":\r\n",
        "                self._process_intune_devices_stage1_data()\r\n",
        "                self._process_intune_devices_stage1_data_for_refined()\r\n",
        "            else:\r\n",
        "                logger.info(\"No defined function for processing this queried data\")\r\n",
        "        \r\n",
        "        logger.info(\"Finished processing graphapi data from stage 1 to stage 2\")\r\n",
        "\r\n",
        "    def _process_intune_devices_stage1_data(self):\r\n",
        "        \"\"\" Process intune devices data from stage 1 without data enrichment \"\"\"\r\n",
        "        source_path = f'{self.stage1np}/devices'\r\n",
        "        logger.info(\"Processing intune devices data without enrichment\")\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        df = spark.readStream.csv(self.stage1np_devices + '/**/*.csv', header='true')\r\n",
        "        #df = df.dropDuplicates(['DeviceName'])\r\n",
        "        # rename column names to use camel case (kind of); remove spaces and hyphens\r\n",
        "        df = df.withColumnRenamed('Device name', 'DeviceName').withColumnRenamed('Managed by', 'ManagedBy').withColumnRenamed('OS version', 'OSVersion')\r\n",
        "        df = df.withColumnRenamed('Last check-in', 'LastCheckIn').withColumnRenamed('Primary user UPN', 'PrimaryUserUPN').withColumnRenamed('Device ID', 'DeviceID')\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        devices_spark_schema = oea.to_spark_schema(self.schemas['devices'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['devices'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/devices_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/devices_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query2.lastProgress)       \r\n",
        "\r\n",
        "    def _process_intune_devices_stage1_data_for_refined(self):\r\n",
        "        \"\"\" Processes intune devices data from stage 1 with added columns\"\"\"\r\n",
        "        source_path = f'{self.stage1np}/devices'\r\n",
        "        logger.info(\"Processing intune devices enriched data\")\r\n",
        "        \r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        df = spark.readStream.csv(self.stage1np_devices + '/**/*.csv', header='true')\r\n",
        "        #df = df.dropDuplicates(['DeviceName'])\r\n",
        "        # rename column names to use camel case (kind of); remove spaces and hyphens\r\n",
        "        df = df.withColumnRenamed('Device name', 'DeviceName').withColumnRenamed('Managed by', 'ManagedBy').withColumnRenamed('OS version', 'OSVersion')\r\n",
        "        df = df.withColumnRenamed('Last check-in', 'LastCheckIn').withColumnRenamed('Primary user UPN', 'PrimaryUserUPN').withColumnRenamed('Device ID', 'DeviceID')\r\n",
        "        # creates two columns based on last check in: last check in date and last check in hour-of-day\r\n",
        "        df = df.withColumn('LastCheckInTime', F.split(F.col('LastCheckIn'), ' ').getItem(1))\r\n",
        "        df = df.withColumn('LastCheckInDate', F.split(F.col('LastCheckIn'), ' ').getItem(0))\r\n",
        "        df = df.withColumn('LastCheckInHourOfDay', F.split(F.col('LastCheckInTime'), ':').getItem(0))\r\n",
        "        df = df.drop('LastCheckInTime')\r\n",
        "            # cast the last check in date column as a date, and create a column identifying the last day of week (Mon, Tues, etc.) \r\n",
        "        df.select(F.col('LastCheckInDate'), F.to_date(F.col('LastCheckInDate'), 'yyyy-MM-dd'))\r\n",
        "        df = df.withColumn('LastCheckInDayOfWeek', F.date_format(F.col('LastCheckIn'), \"E\"))\r\n",
        "            # create new column to identify whether the student/device has access outside of school based on the last check in date and hour of day\r\n",
        "            # currently identifies if the last check in is on a weekend, before 9 AM, or after 4 PM - then the device is considered to have access outside of school\r\n",
        "        df = df.withColumn('AccessOutsideOfSchool', F.when(F.col('LastCheckInDayOfWeek') == \"Sat\", \"true\").otherwise(F.when(F.col('LastCheckInDayOfWeek') == \"Sun\", \"true\").otherwise(F.when(F.col('LastCheckInHourOfDay') >= 16, \"true\").otherwise(F.when(F.col('LastCheckInHourOfDay') < 9, \"true\").otherwise(\"false\")))))\r\n",
        "        # Can comment out this drop if you don't want to drop these two columns (note: you will need to update the schema to reflect this, if you comment this out)\r\n",
        "        df = df.drop('LastCheckInDayOfWeek').drop('LastCheckInHourOfDay')\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        devices_spark_schema = oea.to_spark_schema(self.schemas['devices_enriched'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['devices_enriched'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_p_refined').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/devices_pseudo_refined')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            query.explain()\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_np_refined').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/devices_lookup_refined')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query2.lastProgress)       \r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}