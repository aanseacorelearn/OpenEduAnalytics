{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Review [an example notebook](https://github.com/microsoft/OpenEduAnalytics/blob/main/modules/module_catalog/Microsoft_Education_Insights/notebook/Insights_py.ipynb) on how to load data from the data lake, write back to it and create the Spark databases that will be connected to PowerBI. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Sample Notebook\n",
        "\n",
        "Brief description of the purpose of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Provision Storage Accounts\n",
        "\n",
        "The storage account variable has to be changed to the name of the storage account associated with your Azure resource group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N.B: All the steps below (loading raw data from the lake, preprocessing and aggregating, writing back to the lake and loading to the SparkDB) have to repeated for each new table you will be creating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table Name\n",
        "Description of table\n",
        "\n",
        "**Databases and tables used:**\n",
        "\n",
        "List of databases and tables used.\n",
        "\n",
        "**Databases and tables created:**\n",
        "\n",
        "List of databases and tables created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Raw Data from Lake\n",
        "\n",
        "To ensure that that the right tables are loaded, confirm that the file paths match your data lake storage containers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing and Aggregation\n",
        "\n",
        "All the various stages in transforming, aggregating, exploring and enriching the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write Data Back to Lake\n",
        "\n",
        "After all preprocessing and aggregations steps are complete, write back the data to the data lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load to Spark DB\n",
        "\n",
        "Load the data from the data lake into the Spark DB for connection to PowerBI and easier querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
