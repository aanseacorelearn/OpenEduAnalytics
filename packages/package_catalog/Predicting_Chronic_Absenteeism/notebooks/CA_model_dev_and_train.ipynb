{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Chronic Absenteeism Package: Develop and Train ModelResults Table, using StudentModel Table\r\n",
        "This notebook is intended to explore the capabilities of the OEA Chronic Absenteeism package by developing and training an InterpretML glassbox ML model. \r\n",
        "\r\n",
        "**It is recommended that you review and execute all relevant module pipelines, before testing this Chronic Absenteeism notebook.**\r\n",
        "\r\n",
        " - Train Model Table:\r\n",
        "     * Uses InterpretML ExplainableBoostingClassifier.\r\n",
        "     * Provides a mean-value feature importance visual for visualizing the comparative feature correlations found from the ML model training.\r\n",
        "\r\n",
        "This notebook trains and develops the ML model by:\r\n",
        " 1. Training the ML model on the StudentModel table.\r\n",
        " 2. Refine which specific features are wanted, and developing the InterpretML model predictions table.\r\n",
        " 3. Extracting the top 5 feature-drivers of predicted chronic absenteeism.\r\n",
        " 4. Write this final \"ModelResults_pseudo\" table out to stage 3p.\r\n",
        "\r\n",
        "**NOTE: This notebook must be attached to a spark pool with the proper requirements.txt file.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 0) Initialize the OEA framework.\r\n",
        "oea = OEA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import Relevant Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from interpret.glassbox import ExplainableBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read in the StudentModel table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfStudentModel = oea.load_delta('stage3p/chronic_absenteeism/StudentModel_pseudo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.) Train ML Model on StudentModel table\r\n",
        "\r\n",
        "Training is completed on InterpretML's ExplainableBoostingClassifier; see documentation here: https://interpret.ml/docs/ebm.html. For debugging and questions on the model, see their FAQ page: https://interpret.ml/docs/faq.html#.\r\n",
        "\r\n",
        "To see their GitHub repository with technical explanations, visit: https://github.com/interpretml/interpret. Or for the `ebm.py` script (which uses most of the functions used from InterpretML), visit: https://github.com/interpretml/interpret/blob/develop/python/interpret-core/interpret/glassbox/ebm/ebm.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# select only relevant columns\r\n",
        "dfStudentModel = dfStudentModel.drop('Surname', 'GivenName', 'MiddleName', 'InPerson_numDaysAttended', 'InPerson_percentDaysAttended', 'StudentGrade', 'SchoolName')\r\n",
        "display(dfStudentModel.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pdf = dfStudentModel.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# cross validation accuracy check, used for model exploration.\r\n",
        "\r\n",
        "df = pdf.copy()\r\n",
        "labels = df.pop(\"InPerson_chronicAbsFlag\")\r\n",
        "X = df.copy()\r\n",
        "\r\n",
        "clf = ExplainableBoostingClassifier()\r\n",
        "\r\n",
        "scoring = {'acc': 'accuracy',\r\n",
        "           'f1_macro': 'f1_macro'}\r\n",
        "\r\n",
        "scores = cross_validate(clf, X, labels, cv=5, scoring=scoring)\r\n",
        "\r\n",
        "acc_scores = scores['test_acc']\r\n",
        "f1_scores = scores['test_f1_macro']\r\n",
        "print(\"Classifier: %0.3f Accuracy with a standard deviation of %0.4f\" % (acc_scores.mean(), acc_scores.std()))\r\n",
        "print(\"Classifier: %0.3f macro F1-Score with a standard deviation of %0.4f\\n\" % (f1_scores.mean(), f1_scores.std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# full data model train and explain\r\n",
        "clf.fit(X, labels)\r\n",
        "output = clf.explain_local(X, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "clf.explain_global().visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.) Refine which specific features are wanted, and develop the final InterpretML model predictions table\r\n",
        "\r\n",
        "**NOTES:** \r\n",
        " - Update the features cross-correlated columns to pull the most relevant columns, for processing in the Power BI dashboard analyses.\r\n",
        " - You may also want to push the StudentIds through the training of the model. To do so, look up documentation here: https://interpret.ml/docs/ebm.html\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# student predictions and explanations\r\n",
        "\r\n",
        "score_df_list = []\r\n",
        "features = pd.Index(list(df)+['Clever_avgNumAccessesPerDay_Newsela x Clever_avgNumAccessesPerDay_Office365', 'Clever_avgNumAccessesPerDay_DestinyDiscover x Clever_avgNumAccessesPerDay_XtraMath', \\\r\n",
        "'Clever_avgNumAccessesPerDay_Scholastic x Clever_avgNumAccessesPerDay_SpringBoard', 'Clever_avgNumAccessesPerDay_Office365 x Clever_avgNumAccessesPerDay_Scholastic', \\\r\n",
        "'Clever_avgNumAccessesPerDay_EdgenuityCourseware x Clever_avgNumAccessesPerDay_Office365'])\r\n",
        "\r\n",
        "for i in range(len(X)):\r\n",
        "    datapoint = output.data(key=i)\r\n",
        "    score_dict = {}\r\n",
        "    for idx, column_name in enumerate(features):\r\n",
        "        if (column_name == 'StudentId_external_pseudonym'):\r\n",
        "            score_dict[column_name] = datapoint['values'][idx]\r\n",
        "        else:\r\n",
        "            #NOTE: uncomment the next line if you want all features to have 'interpret_val' appended to the column name, and comment out line 17 \r\n",
        "            #score_dict[column_name + \"_interpret_val\"] = datapoint['scores'][idx]\r\n",
        "            score_dict[column_name] = datapoint['scores'][idx]\r\n",
        "    score_dict['true_class'] = datapoint['perf']['actual']\r\n",
        "    score_dict['predicted_class'] = datapoint['perf']['predicted']\r\n",
        "    score_dict['predicted_score'] = datapoint['perf']['predicted_score']\r\n",
        "\r\n",
        "    if score_dict['predicted_class'] == 0:\r\n",
        "        score_dict['absentee_probability'] = 1-datapoint['perf']['predicted_score']\r\n",
        "    else:\r\n",
        "        score_dict['absentee_probability'] = datapoint['perf']['predicted_score']\r\n",
        "    score_df_list.append(score_dict)\r\n",
        "\r\n",
        "explainable_df = pd.DataFrame(score_df_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "explainable_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.) Extract the top 5 Drivers of Predicted Chronic Absenteeism\r\n",
        "\r\n",
        "The first step to this extraction is taking the absolute value of each potential driver column. Negative driver-scores imply raw data values that correspond to (predicted) preventative factors of chronic absenteeism. Thus, taking the absolute value indicates the absolute weight of the drivers. \r\n",
        "\r\n",
        "The original `explainable_df` is then melted into a column per feature name, and feature value - for the top 5 feature drivers.\r\n",
        "\r\n",
        "The last code block then extracts the \"true_class\", \"predicted_class\", \"predicted_score\", and \"absentee_probability\" columns from `explainable_df` and adds them to the final `dfModelResults_features` table.\r\n",
        "\r\n",
        "The final `dfModelResults_features` table has the dimensions: (the number of students) x (15 columns)\r\n",
        "\r\n",
        "**NOTE:** If you'd like to keep all drivers and their weights, you can edit or remove the code-blocks in this step, as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "\r\n",
        "explainable_df_melted = explainable_df.drop('true_class', axis=1).drop('predicted_class', axis=1).drop('predicted_score', axis=1).drop('absentee_probability', axis=1)\r\n",
        "\r\n",
        "list_explainable_df_features = explainable_df_melted.drop('StudentId_external_pseudonym', axis=1)\r\n",
        "list_explainable_df_features = list_explainable_df_features.columns.tolist()\r\n",
        "# melt the df and create the absolute value table\r\n",
        "explainable_df_melted = explainable_df_melted.melt(id_vars = ['StudentId_external_pseudonym'],value_vars = list_explainable_df_features,var_name='feature',value_name='feature_interpret_val')\r\n",
        "explainable_df_melted_abs = explainable_df_melted.copy()\r\n",
        "explainable_df_melted_abs['feature_interpret_val'] = explainable_df_melted_abs['feature_interpret_val'].abs()\r\n",
        "# push these tables to pyspark dfs\r\n",
        "dfModelResults_melted = spark.createDataFrame(explainable_df_melted)\r\n",
        "dfModelResults_melted = dfModelResults_melted.withColumnRenamed('StudentId_external_pseudonym', 'StudentId')\r\n",
        "dfModelResults_melted_abs = spark.createDataFrame(explainable_df_melted_abs)\r\n",
        "# extract the first/top feature based on the interpretML value per student \r\n",
        "dfModelResults_f1 = dfModelResults_melted_abs.groupBy('StudentId_external_pseudonym').max('feature_interpret_val')\r\n",
        "dfModelResults_f1 = dfModelResults_f1.withColumnRenamed('StudentId_external_pseudonym', 'StudentId').withColumnRenamed('max(feature_interpret_val)', 'feature_val')\r\n",
        "# create features df and grab the feature name\r\n",
        "dfModelResults_features = dfModelResults_f1.join(dfModelResults_melted_abs, (dfModelResults_f1.StudentId == dfModelResults_melted_abs.StudentId_external_pseudonym)\\\r\n",
        " & (dfModelResults_f1.feature_val == dfModelResults_melted_abs.feature_interpret_val), how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId', 'feature_val')\r\n",
        "dfModelResults_features = dfModelResults_features.withColumnRenamed('feature', 'feature1').withColumnRenamed('feature_interpret_val', 'feature1_interpret_absVal')\r\n",
        "# grab the actual, model-assigned value for the feature\r\n",
        "dfModelResults_features = dfModelResults_features.join(dfModelResults_melted, (dfModelResults_features.StudentId_external_pseudonym == dfModelResults_melted.StudentId)\\\r\n",
        " & (dfModelResults_features.feature1 == dfModelResults_melted.feature), how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId', 'feature', 'feature1_interpret_absVal').withColumnRenamed('feature_interpret_val', 'feature1_interpret_val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# remove the rows already in the dfModelResults_features from the melted_abs table to find the next top feature\r\n",
        "dfModelResults_melted_abs = dfModelResults_melted_abs.withColumnRenamed('StudentId_external_pseudonym', 'StudentId_external')\r\n",
        "dfModelResults_melted_abs = dfModelResults_melted_abs.join(dfModelResults_features, (dfModelResults_melted_abs.StudentId_external == dfModelResults_features.StudentId_external_pseudonym)\\\r\n",
        " & (dfModelResults_melted_abs.feature == dfModelResults_features.feature1), how='leftanti')\r\n",
        "# extract the second-top feature based on the interpretML value per student \r\n",
        "dfModelResults_f2 = dfModelResults_melted_abs.groupBy('StudentId_external').max('feature_interpret_val')\r\n",
        "dfModelResults_f2 = dfModelResults_f2.withColumnRenamed('StudentId_external', 'StudentId').withColumnRenamed('max(feature_interpret_val)', 'feature_val')\r\n",
        "# grab the feature name\r\n",
        "dfModelResults_f2 = dfModelResults_f2.join(dfModelResults_melted_abs, (dfModelResults_f2.StudentId == dfModelResults_melted_abs.StudentId_external) \\\r\n",
        "& (dfModelResults_f2.feature_val == dfModelResults_melted_abs.feature_interpret_val), how='inner')\r\n",
        "dfModelResults_f2 = dfModelResults_f2.drop('StudentId', 'feature_val')\r\n",
        "dfModelResults_f2 = dfModelResults_f2.withColumnRenamed('feature', 'feature2').withColumnRenamed('feature_interpret_val', 'feature2_interpret_absVal')\r\n",
        "# grab the actual, model-assigned value for the feature\r\n",
        "dfModelResults_f2 = dfModelResults_f2.join(dfModelResults_melted, (dfModelResults_f2.StudentId_external == dfModelResults_melted.StudentId)\\\r\n",
        " & (dfModelResults_f2.feature2 == dfModelResults_melted.feature), how='inner')\r\n",
        "dfModelResults_f2 = dfModelResults_f2.drop('StudentId', 'feature', 'feature2_interpret_absVal').withColumnRenamed('feature_interpret_val', 'feature2_interpret_val')\r\n",
        "# join feature 2 table to the final dfModelResults_features table\r\n",
        "dfModelResults_features = dfModelResults_features.join(dfModelResults_f2, dfModelResults_features.StudentId_external_pseudonym == dfModelResults_f2.StudentId_external, how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId_external')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# remove the rows already in the dfModelResults_features from the melted_abs table to find the next top feature\r\n",
        "dfModelResults_melted_abs = dfModelResults_melted_abs.join(dfModelResults_features, (dfModelResults_melted_abs.StudentId_external == dfModelResults_features.StudentId_external_pseudonym)\\\r\n",
        " & (dfModelResults_melted_abs.feature == dfModelResults_features.feature2), how='leftanti')\r\n",
        "# extract the third-top feature based on the interpretML value per student \r\n",
        "dfModelResults_f3 = dfModelResults_melted_abs.groupBy('StudentId_external').max('feature_interpret_val')\r\n",
        "dfModelResults_f3 = dfModelResults_f3.withColumnRenamed('StudentId_external', 'StudentId').withColumnRenamed('max(feature_interpret_val)', 'feature_val')\r\n",
        "# grab the feature name\r\n",
        "dfModelResults_f3 = dfModelResults_f3.join(dfModelResults_melted_abs, (dfModelResults_f3.StudentId == dfModelResults_melted_abs.StudentId_external) \\\r\n",
        "& (dfModelResults_f3.feature_val == dfModelResults_melted_abs.feature_interpret_val), how='inner')\r\n",
        "dfModelResults_f3 = dfModelResults_f3.drop('StudentId', 'feature_val')\r\n",
        "dfModelResults_f3 = dfModelResults_f3.withColumnRenamed('feature', 'feature3').withColumnRenamed('feature_interpret_val', 'feature3_interpret_absVal')\r\n",
        "# grab the actual, model-assigned value for the feature\r\n",
        "dfModelResults_f3 = dfModelResults_f3.join(dfModelResults_melted, (dfModelResults_f3.StudentId_external == dfModelResults_melted.StudentId)\\\r\n",
        " & (dfModelResults_f3.feature3 == dfModelResults_melted.feature), how='inner')\r\n",
        "dfModelResults_f3 = dfModelResults_f3.drop('StudentId', 'feature', 'feature3_interpret_absVal').withColumnRenamed('feature_interpret_val', 'feature3_interpret_val')\r\n",
        "# join feature 3 table to the final dfModelResults_features table\r\n",
        "dfModelResults_features = dfModelResults_features.join(dfModelResults_f3, dfModelResults_features.StudentId_external_pseudonym == dfModelResults_f3.StudentId_external, how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId_external')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# remove the rows already in the dfModelResults_features from the melted_abs table to find the next top feature\r\n",
        "dfModelResults_melted_abs = dfModelResults_melted_abs.join(dfModelResults_features, (dfModelResults_melted_abs.StudentId_external == dfModelResults_features.StudentId_external_pseudonym)\\\r\n",
        " & (dfModelResults_melted_abs.feature == dfModelResults_features.feature3), how='leftanti')\r\n",
        "# extract the fourth-top feature based on the interpretML value per student \r\n",
        "dfModelResults_f4 = dfModelResults_melted_abs.groupBy('StudentId_external').max('feature_interpret_val')\r\n",
        "dfModelResults_f4 = dfModelResults_f4.withColumnRenamed('StudentId_external', 'StudentId').withColumnRenamed('max(feature_interpret_val)', 'feature_val')\r\n",
        "# grab the feature name\r\n",
        "dfModelResults_f4 = dfModelResults_f4.join(dfModelResults_melted_abs, (dfModelResults_f4.StudentId == dfModelResults_melted_abs.StudentId_external) \\\r\n",
        "& (dfModelResults_f4.feature_val == dfModelResults_melted_abs.feature_interpret_val), how='inner')\r\n",
        "dfModelResults_f4 = dfModelResults_f4.drop('StudentId', 'feature_val')\r\n",
        "dfModelResults_f4 = dfModelResults_f4.withColumnRenamed('feature', 'feature4').withColumnRenamed('feature_interpret_val', 'feature4_interpret_absVal')\r\n",
        "# grab the actual, model-assigned value for the feature\r\n",
        "dfModelResults_f4 = dfModelResults_f4.join(dfModelResults_melted, (dfModelResults_f4.StudentId_external == dfModelResults_melted.StudentId)\\\r\n",
        " & (dfModelResults_f4.feature4 == dfModelResults_melted.feature), how='inner')\r\n",
        "dfModelResults_f4 = dfModelResults_f4.drop('StudentId', 'feature', 'feature4_interpret_absVal').withColumnRenamed('feature_interpret_val', 'feature4_interpret_val')\r\n",
        "# join feature 4 table to the final dfModelResults_features table\r\n",
        "dfModelResults_features = dfModelResults_features.join(dfModelResults_f4, dfModelResults_features.StudentId_external_pseudonym == dfModelResults_f4.StudentId_external, how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId_external')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# remove the rows already in the dfModelResults_features from the melted_abs table to find the next top feature\r\n",
        "dfModelResults_melted_abs = dfModelResults_melted_abs.join(dfModelResults_features, (dfModelResults_melted_abs.StudentId_external == dfModelResults_features.StudentId_external_pseudonym)\\\r\n",
        " & (dfModelResults_melted_abs.feature == dfModelResults_features.feature4), how='leftanti')\r\n",
        "# extract the fifth-top feature based on the interpretML value per student \r\n",
        "dfModelResults_f5 = dfModelResults_melted_abs.groupBy('StudentId_external').max('feature_interpret_val')\r\n",
        "dfModelResults_f5 = dfModelResults_f5.withColumnRenamed('StudentId_external', 'StudentId').withColumnRenamed('max(feature_interpret_val)', 'feature_val')\r\n",
        "# grab the feature name\r\n",
        "dfModelResults_f5 = dfModelResults_f5.join(dfModelResults_melted_abs, (dfModelResults_f5.StudentId == dfModelResults_melted_abs.StudentId_external) \\\r\n",
        "& (dfModelResults_f5.feature_val == dfModelResults_melted_abs.feature_interpret_val), how='inner')\r\n",
        "dfModelResults_f5 = dfModelResults_f5.drop('StudentId', 'feature_val')\r\n",
        "dfModelResults_f5 = dfModelResults_f5.withColumnRenamed('feature', 'feature5').withColumnRenamed('feature_interpret_val', 'feature5_interpret_absVal')\r\n",
        "# grab the actual, model-assigned value for the feature\r\n",
        "dfModelResults_f5 = dfModelResults_f5.join(dfModelResults_melted, (dfModelResults_f5.StudentId_external == dfModelResults_melted.StudentId)\\\r\n",
        " & (dfModelResults_f5.feature5 == dfModelResults_melted.feature), how='inner')\r\n",
        "dfModelResults_f5 = dfModelResults_f5.drop('StudentId', 'feature', 'feature5_interpret_absVal').withColumnRenamed('feature_interpret_val', 'feature5_interpret_val')\r\n",
        "# join feature 5 table to the final dfModelResults_features table\r\n",
        "dfModelResults_features = dfModelResults_features.join(dfModelResults_f5, dfModelResults_features.StudentId_external_pseudonym == dfModelResults_f5.StudentId_external, how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId_external')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#print((dfModelResults_features.count(), len(dfModelResults_features.columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# join the model results of true classification, predicted classification, predicted score, and absentee probability to the final model results table\r\n",
        "dfModelResults = spark.createDataFrame(explainable_df)\r\n",
        "dfModelResults = dfModelResults.select('StudentId_external_pseudonym', 'true_class', 'predicted_class', 'predicted_score', 'absentee_probability')\r\n",
        "dfModelResults = dfModelResults.withColumnRenamed('StudentId_external_pseudonym', 'StudentId')\r\n",
        "dfModelResults_features = dfModelResults_features.join(dfModelResults, dfModelResults_features.StudentId_external_pseudonym == dfModelResults.StudentId, how='inner')\r\n",
        "dfModelResults_features = dfModelResults_features.drop('StudentId')\r\n",
        "# clean column names to write to stage 3 per delta file requirements\r\n",
        "#dfModelResults = dfModelResults.select([F.col(col).alias(col.replace(' ', '')) for col in dfModelResults.columns])\r\n",
        "display(dfModelResults_features.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4.) Write this final \"ModelResults_pseudo\" table out to stage 3p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfModelResults_features.coalesce(1).write.format('delta').mode('overwrite').option('header', True).save(oea.stage3p + '/chronic_absenteeism/ModelResults_pseudo')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}