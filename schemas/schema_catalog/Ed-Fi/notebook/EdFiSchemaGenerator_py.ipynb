{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TestSparkApp\")\\\n",
        "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\\\n",
        "        .appName('MDESparkApp')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "class EdFiSchemaGenerator:\n",
        "    \"\"\"\n",
        "    A class to Read Metadata file and Generate Ed-Fi Schema in Delta format.\n",
        "    Attributes:\n",
        "    -----------\n",
        "        metadata_path: Path to the Metadata file.\n",
        "        out_path: Path to store Output Delta tables in Ed-Fi schema.\n",
        "        swagger_url: URL to the Ed-Fi Open API Swagger endpoint.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metadata_path, out_path, swagger_url):\n",
        "        self.metadata_path = metadata_path\n",
        "        self.out_path = out_path\n",
        "        self.metadata_values = spark.read.csv(self.metadata_path, header=True).collect()\n",
        "\n",
        "        self.tables = list(set([x.table_name for x in self.metadata_values]))\n",
        "        self.yet_to_process_tables = [x for x in self.tables]\n",
        "        self.swagger_json = json.loads(requests.get(swagger_url).text)\n",
        "        # self.swagger_json = None\n",
        "        self.processed_tables = []\n",
        "        self.dependency_dict = {}\n",
        "        self.schemas = {}\n",
        "        self.primitive_tables = []\n",
        "\n",
        "    def get_reference(self, row):\n",
        "        flattened_fields = []\n",
        "        if(row.type == 'array'):\n",
        "            reference = (row.items).split('/')[-1][:-1]\n",
        "        elif(row['$ref'] != None):\n",
        "            if('::' in row['$ref']):\n",
        "                reference = row['$ref'].split('::')[0]\n",
        "                flattened_fields = (row['$ref'].split('::')[1]).split(':')\n",
        "            else:\n",
        "                reference = row['$ref']\n",
        "        else:\n",
        "            return None, None\n",
        "        reference = reference.split('/')[-1]\n",
        "        return reference, flattened_fields\n",
        "\n",
        "    def get_data_type(self, dtype, format):\n",
        "        if(dtype == 'string'):\n",
        "            if(format == 'date'):\n",
        "                return DateType()\n",
        "            if(format == 'date-time'):\n",
        "                return TimestampType()\n",
        "            return StringType()\n",
        "        if(dtype == 'integer'):\n",
        "            return IntegerType()\n",
        "        if(dtype == 'number'):\n",
        "            return DecimalType()\n",
        "        if(dtype == 'boolean'):\n",
        "            return BooleanType()\n",
        "\n",
        "    def create_primitive_schemas(self):\n",
        "        print(\"Creating primitive schemas.\")\n",
        "        for table in self.tables:\n",
        "            table_df = [x for x in self.metadata_values if x.table_name == table]\n",
        "            isPrimitive = not(any(x for x in table_df if x['$ref'] != None)) and not(any(x for x in table_df if x.type == 'array'))\n",
        "            if(isPrimitive):\n",
        "                self.schemas[table] = StructType([StructField(row.column_name, self.get_data_type(row.type, row.format), not(row['x-Ed-Fi-isIdentity'])) for row in table_df])\n",
        "                self.primitive_tables.append(table)\n",
        "                self.processed_tables.append(table)\n",
        "                self.yet_to_process_tables.remove(table)\n",
        "        print(\"Completed creating primitive schemas.\")\n",
        "\n",
        "    def create_dependency_dict(self):\n",
        "        referenced_df_values = [x for x in self.metadata_values if x.type == 'array' or x['$ref'] != None]\n",
        "        # referenced_df_values = self.metadata_df.filter((f.col('type') == 'array') | (f.col('$ref').isNotNull())).collect()\n",
        "        for row in referenced_df_values:\n",
        "            reference, flatten_fields = self.get_reference(row)\n",
        "            if(row.table_name in self.dependency_dict.keys() and reference not in self.dependency_dict[row.table_name]):\n",
        "                self.dependency_dict[row.table_name].append(reference)\n",
        "            elif(row not in self.dependency_dict):\n",
        "                self.dependency_dict[row.table_name] = [reference]\n",
        "\n",
        "    def create_schemas_for_complex_tables(self):\n",
        "        print(\"Creating complex schemas.\")\n",
        "        while len(self.yet_to_process_tables) > 0:\n",
        "            for entity in self.yet_to_process_tables:\n",
        "                if(len([x for x in self.dependency_dict[entity] if x not in self.processed_tables]) == 0):\n",
        "                    table_schema = [x for x in self.metadata_values if x.table_name == entity]\n",
        "                    spark_schema = []\n",
        "                    for row in table_schema:\n",
        "                        reference, flatten_fields = self.get_reference(row)\n",
        "                        if(row.type == 'array'):\n",
        "                            # Handle Array Objects\n",
        "                            column_schema = StructField(row.column_name, ArrayType(self.schemas[reference]), True)\n",
        "                        elif(row['$ref'] != None):\n",
        "                            # Handle Normal Objects\n",
        "                            if(len(flatten_fields)>0):\n",
        "                                spark_schema += [field for field in self.schemas[reference].fields if field.name in flatten_fields]\n",
        "                                continue\n",
        "                            else:\n",
        "                                column_schema = StructField(row.column_name, self.schemas[reference], True)\n",
        "                        else:\n",
        "                            # Primitive Data type\n",
        "                            column_schema = StructField(row.column_name, self.get_data_type(row.type, row.format))\n",
        "                        spark_schema.append(column_schema)\n",
        "                    self.schemas[entity] = StructType(spark_schema)\n",
        "                    self.processed_tables.append(entity)\n",
        "                    self.yet_to_process_tables.remove(entity)\n",
        "                    print(f'created schema for {entity}')\n",
        "        print(\"Completed creating complex schemas.\")\n",
        "\n",
        "    def write_empty_delta_files(self):\n",
        "        # with open(\"C:/Users/agundapaneni/Downloads/swagger_edfi.json\") as f:\n",
        "            # self.swagger_json = json.load(f)\n",
        "        for path in self.swagger_json['paths']:\n",
        "            if(path.count('/') == 2):\n",
        "                response = self.swagger_json['paths'][path]['get']['responses']['200']['schema']\n",
        "                reference = response['items']['$ref']\n",
        "                reference = reference.split('_')[-1]\n",
        "                emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD(), self.schemas[reference]).withColumn('LastModifiedDate', f.lit(None).cast(TimestampType()))\n",
        "                emptyDF.write.format('delta').mode('overwrite').save(f\"{self.out_path}/{path.split('/')[-1]}\")\n",
        "\n",
        "    def create_schemas(self):\n",
        "        self.create_primitive_schemas()\n",
        "        self.create_dependency_dict()\n",
        "        self.create_schemas_for_complex_tables()\n",
        "        self.write_empty_delta_files()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('Spark_Dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "save_output": true,
    "vscode": {
      "interpreter": {
        "hash": "be620d6548581d4b255e22136663c988a2c6aa20c2fcc1339f2748495f9e98b2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
